{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.contrib.training.batch_sequences_with_states.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7NmNapeZ5aRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# tensorflow.contrib.training.batch_sequences_with_states (Legacy)"
      ]
    },
    {
      "metadata": {
        "id": "XFgQeNX56Rrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "As is described in the [official documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states), this function creates and batches segments of input sequences. In addition, for each sample in a batch, it maintains and updates a copy of state. This is quite handy when this method is employed to construct data input pipelines for models such as RNN."
      ]
    },
    {
      "metadata": {
        "id": "ByP6C3qrNx7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f2ff265-5d04-457e-a5ed-72380a0567e4"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.training import batch_sequences_with_states\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print('TensorFlow version:', tf.VERSION)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s4url2Nbbalk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating Toy Data\n",
        "The toy data used in the following sections makes up an instance of the **sequence classification**. Credit goes to Jason Brownlee's illuminating [tutorial](https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/) (Example 5).\n",
        "\n",
        "_\"The problem is defined as a sequence of random values between 0 and 1. This sequence is taken as input for the problem with each number provided one per timestep._\n",
        "\n",
        "_A binary label (0 or 1) is associated with each input. The output values are all 0. Once the cumulative sum of the input values in the sequence exceeds a threshold, then the output value flips from 0 to 1._\n",
        "\n",
        "_A threshold of 1/4 the sequence length is used.\"_"
      ]
    },
    {
      "metadata": {
        "id": "o03W_IKQrAS-",
        "colab_type": "code",
        "outputId": "00a8d5c6-5e4e-4df4-9df4-d9f7b20f03ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "list_seqs, list_labels = list(), list()\n",
        "max_seq_len = 15\n",
        "min_seq_len = 12\n",
        "num_seq = 100\n",
        "for _ in range(num_seq):\n",
        "    len_seq = np.random.randint(min_seq_len, max_seq_len)\n",
        "    seq_tokens = np.random.rand(len_seq)\n",
        "    list_seqs.append(seq_tokens)\n",
        "    threshold = len_seq / 4.0\n",
        "    seq_labels = np.array([int(x > threshold) for x in np.cumsum(seq_tokens)])\n",
        "    list_labels.append(seq_labels)\n",
        "\n",
        "# Print the first 5 sequences of tokens and labels\n",
        "for seq, labels in zip(list_seqs[:5], list_labels):\n",
        "    pprint(seq)\n",
        "    pprint(labels)\n",
        "    print('\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([0.39676616, 0.86157441, 0.83979143, 0.67745519, 0.20498841,\n",
            "       0.2182948 , 0.23537936, 0.12690887, 0.14148605, 0.95540276,\n",
            "       0.54239793, 0.39336251])\n",
            "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "\n",
            "array([0.22966606, 0.54282423, 0.48165246, 0.17545146, 0.49743892,\n",
            "       0.96969642, 0.30970795, 0.57493861, 0.36068693, 0.08612079,\n",
            "       0.78507474, 0.52802721, 0.91969071, 0.60057449])\n",
            "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "\n",
            "array([0.13858428, 0.42406717, 0.0669202 , 0.81489591, 0.37393337,\n",
            "       0.94317965, 0.99856732, 0.89698231, 0.48522192, 0.2183444 ,\n",
            "       0.48269078, 0.47967869])\n",
            "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "\n",
            "array([0.22425017, 0.97210877, 0.86698953, 0.2286461 , 0.12806215,\n",
            "       0.24681088, 0.60760401, 0.96746138, 0.49992784, 0.21748109,\n",
            "       0.92627136, 0.33691707, 0.67201917])\n",
            "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "\n",
            "array([0.71588358, 0.95822808, 0.75794044, 0.13306914, 0.13664142,\n",
            "       0.29724938, 0.05303291, 0.42700826, 0.4513843 , 0.96192315,\n",
            "       0.42002514, 0.56725873])\n",
            "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-VVYJp1ckSm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id='writing_to_tfrecords'></a>\n",
        "## Writing to TFRecords\n",
        "\n",
        "### Thread-and-queue-based implementation\n",
        "\n",
        "Instead of directly accepting the sequence data, the `input_sequences` argument of `batch_sequences_with_states` requires thread-and-queue-based implementation of the input sequences. Unfortunately, this crucial requirement is either ambiguously expressed or totally missing in the documentation. A main consequence of falling into this pitfall is getting duplicates of the same sequence of data. For an example, see [this issue](https://github.com/tensorflow/tensorflow/issues/21959).\n",
        "\n",
        "###  `tf.train.SequenceExample`\n",
        "\n",
        "In order to prepare the input for `batch_sequences_with_states`, the data can be first wrapped by `tf.train.SequenceExample`, which is essentially a [protocal buffer](https://developers.google.com/protocol-buffers/?hl=en). To have a glance at concrete examples of `SequenceExample`, it may suffice to look at the examples coming with the [definition](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/core/example/example.proto). The code in the following cell instantiates a naive usage of this data wrapper."
      ]
    },
    {
      "metadata": {
        "id": "TVAJ72UEi0CM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "F_TOKEN_ID = 'token'\n",
        "F_LABEL_ID = 'label'\n",
        "fname_data = 'toydata.tfrecords'\n",
        "if os.path.isfile(fname_data):\n",
        "    os.remove(fname_data)\n",
        "    \n",
        "writer = tf.python_io.TFRecordWriter(fname_data)\n",
        "for seq_tokens, seq_labels in zip(list_seqs, list_labels):\n",
        "    seq = tf.train.SequenceExample()\n",
        "    flist = seq.feature_lists.feature_list\n",
        "    tokens = flist[F_TOKEN_ID].feature\n",
        "    labels = flist[F_LABEL_ID].feature\n",
        "    for tk, lb in zip(seq_tokens, seq_labels):\n",
        "        token = tokens.add()\n",
        "        token.float_list.value.append(tk)\n",
        "        label = labels.add()\n",
        "        label.int64_list.value.append(lb)\n",
        "    writer.write(seq.SerializeToString())\n",
        "\n",
        "# Dont' forget to close the writer!!!\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6FA0AVzorbMD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "\n",
        "Three classes / functions are involved in the loading and parsing of the toy data: `tf.train.string_input_producer`,  `tf.TFRecordReader` and `tf.parse_single_sequence_example`.\n",
        "\n",
        "### `tf.train.string_input_producer`\n",
        "According to the [documentation](https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer), this function creates a queue which outputs the string values passed to this function for data input pipelines. A typical usage of this function is outputting filenames. In addition, when this function is called, a `QueueRunner` is added to the containing graph's `QUEUE_RUNNER` collection. Therefore, the user does not need to manually add a `QueueRunner`.\n",
        "\n",
        "### `tf.TFRecordReader`\n",
        "Reads the data stored in a `tfrecords` file.\n",
        "\n",
        "### `tf.parse_single_sequence_example`\n",
        "Parses a serialized `SequenceExample`. Apart from the serialized record, the main argument is `sequence_features` which specifies the characteristic information about each contained feature, as is identified by the id / key in the `feature_list` of `SequenceExample`. In this toy scenario, the only feature at issue is a 1-D scalar, namely, an integer, which is identified by the key `F_TOKEN_ID`. Therefore, it suffices to characterize the feature to parse with `tf.FixedLenSequenceFeature([], dtype=tf.int64)`.\n"
      ]
    },
    {
      "metadata": {
        "id": "_cZxcJAApw3z",
        "colab_type": "code",
        "outputId": "d38b29e2-6fed-4095-da94-4d710975a035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "file_queue = tf.train.string_input_producer([fname_data])\n",
        "reader = tf.TFRecordReader()\n",
        "seq_key, serialized_record = reader.read(file_queue)\n",
        "ctx, sequence = tf.parse_single_sequence_example(\n",
        "    serialized_record,\n",
        "    sequence_features={\n",
        "        F_TOKEN_ID: tf.FixedLenSequenceFeature([], dtype=tf.float32),\n",
        "        F_LABEL_ID: tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
        "    })"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-ff18efce15ad>:1: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From <ipython-input-4-ff18efce15ad>:2: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T_m_Cu5yNiqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's first output the sequence keys, sequence values and label values loaded from the `tfrecords` file."
      ]
    },
    {
      "metadata": {
        "id": "XuSyEuSWb3Yp",
        "colab_type": "code",
        "outputId": "949feef5-178b-4730-b7e3-dfa849ef37be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.train.MonitoredSession() as sess:\n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "    for _ in range(5):\n",
        "        val_seq_key, val_seq = sess.run([seq_key, sequence])\n",
        "        print(val_seq_key.decode())\n",
        "        print('tokens:\\t', val_seq[F_TOKEN_ID])\n",
        "        print('labels:\\t', val_seq[F_LABEL_ID], '\\n')\n",
        "    coord.request_stop()\n",
        "    coord.join(threads)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "toydata.tfrecords:0\n",
            "tokens:\t [0.39676616 0.8615744  0.8397914  0.6774552  0.2049884  0.2182948\n",
            " 0.23537935 0.12690887 0.14148605 0.9554028  0.5423979  0.39336252]\n",
            "labels:\t [0 0 0 0 0 1 1 1 1 1 1 1] \n",
            "\n",
            "toydata.tfrecords:246\n",
            "tokens:\t [0.22966605 0.5428242  0.48165247 0.17545146 0.4974389  0.9696964\n",
            " 0.30970794 0.5749386  0.36068693 0.08612079 0.7850748  0.52802724\n",
            " 0.9196907  0.6005745 ]\n",
            "labels:\t [0 0 0 0 0 0 0 1 1 1 1 1 1 1] \n",
            "\n",
            "toydata.tfrecords:527\n",
            "tokens:\t [0.13858429 0.42406717 0.06692021 0.8148959  0.37393337 0.94317967\n",
            " 0.99856734 0.8969823  0.48522192 0.2183444  0.48269078 0.4796787 ]\n",
            "labels:\t [0 0 0 0 0 0 1 1 1 1 1 1] \n",
            "\n",
            "toydata.tfrecords:773\n",
            "tokens:\t [0.22425017 0.9721088  0.86698955 0.2286461  0.12806216 0.24681088\n",
            " 0.607604   0.9674614  0.49992785 0.21748109 0.9262714  0.33691707\n",
            " 0.6720192 ]\n",
            "labels:\t [0 0 0 0 0 0 1 1 1 1 1 1 1] \n",
            "\n",
            "toydata.tfrecords:1037\n",
            "tokens:\t [0.71588355 0.95822805 0.7579405  0.13306914 0.13664143 0.29724938\n",
            " 0.05303291 0.42700827 0.4513843  0.9619232  0.42002514 0.5672587 ]\n",
            "labels:\t [0 0 0 0 0 0 1 1 1 1 1 1] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d-C_SpYvsEp9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Batch Sequence with States\n",
        "\n",
        "### `initial_states`\n",
        "As a characteristic feature of `tf.contrib.training.batch_sequences_with_states`, a copy of states for each input sequence is maintained. Therefore, it contains the variable `initial_states` requiring a dict mapping state names (of string type) to initial values of the states. Specifically, the values of `initial_states` should be in line with the RNN model to combine with. Since the LSTM layer used later is built from `tf.nn.static_state_saving_rnn` using `tf.nn.rnn_cell.LSTMCell`, two states are involved: `'lstm_state_c'` and `'lstm_state_h'`.\n",
        "\n",
        "For the purpose of readability, a small `lstm_size` is temporarily used, which will be revised later when training a toy LSTM model.\n",
        "\n",
        "### `make_keys_unique`\n",
        "In addition, the argument `make_keys_unique` needs to be set to True, in order to repeatedly use the data in training. Formally, as the documentation for this function states, it appends a random integer to the end of `input_key`. This effect can be viewed below. As a result, the value of `input_key` for an input sequence is different every time when the sequence is fed to the model."
      ]
    },
    {
      "metadata": {
        "id": "6dxYqUACsK9f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm_size = 8\n",
        "num_unroll = 3\n",
        "states =  {\n",
        "    'lstm_state_c': np.random.rand(lstm_size),\n",
        "    'lstm_state_h': np.random.rand(lstm_size)\n",
        "}\n",
        "batch_size = 2\n",
        "batch = tf.contrib.training.batch_sequences_with_states(\n",
        "    input_key=seq_key,\n",
        "    input_sequences=sequence,\n",
        "    input_context=ctx,\n",
        "    input_length=tf.shape(sequence[F_TOKEN_ID])[0],\n",
        "    initial_states=states,\n",
        "    num_unroll=num_unroll,\n",
        "    batch_size=batch_size,\n",
        "    allow_small_batch=False,\n",
        "    make_keys_unique=True,\n",
        "    make_keys_unique_seed=29392\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IizFNLm1iXwS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A trivial update of the states is used. Specifically, each scalar component is added 1. Looking at the values of the tensors involved in the function `batch_sequences_with_states`, the following properties can be seen.\n",
        "\n",
        "### `batch.key`\n",
        "\n",
        "The value of each segment's `key` is a string built by joining three components with colon.\n",
        "\n",
        "* The number of segments obtained from an input sequence and the index of the current segment (e.g., `'00000_of_00005'`).\n",
        "* The TFRecords file name.\n",
        "* The random integer identifying the input sequence in the current iteration.\n",
        "\n",
        "### `batch.state`\n",
        "\n",
        "Echoing the unique random integer contained in the value of `key`, a fresh copy of states is created and updated.\n",
        "That is, if an input sequence is fetched N times in the session, N copies of states will be created.\n",
        "This can be seen if a small value of `num_seq` is picked and more iterations are run.\n",
        "\n",
        "### Zero padding\n",
        "\n",
        "When the length of an input sequence is not a multiple of `num_unroll`, zeros are padded to the last segment.\n"
      ]
    },
    {
      "metadata": {
        "id": "wsTudFGgtV6G",
        "colab_type": "code",
        "outputId": "d4887724-7ec0-402f-fd6f-8f0f125d5a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3141
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    state_c = batch.state('lstm_state_c')\n",
        "    state_h = batch.state('lstm_state_h')\n",
        "    update_state_c = batch.save_state('lstm_state_c', state_c + 1)\n",
        "    update_state_h = batch.save_state('lstm_state_h', state_h + 1)\n",
        "    update_states = tf.group(update_state_c, update_state_h)\n",
        "    coord = tf.train.Coordinator()\n",
        "    tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "    for i in range(8):\n",
        "        print('Iteration {}:'.format(i + 1))\n",
        "        val_key, val_next_key, val_seqs, val_labels, val_state_c, val_state_h, \\\n",
        "        val_batch_len, _ = sess.run([\n",
        "            batch.key, batch.next_key, batch.sequences[F_TOKEN_ID],\n",
        "            batch.sequences[F_LABEL_ID], state_c, state_h,\n",
        "            batch.length, update_states\n",
        "        ])\n",
        "        for name, val in zip(\n",
        "            ['keys','sequences', 'labels', 'state_c', 'state_h'],\n",
        "            [val_key, val_seqs, val_labels, val_state_c, val_state_h]\n",
        "        ):\n",
        "            print(name, '\\n', val)\n",
        "        print('\\n')\n",
        "    coord.request_stop()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1:\n",
            "keys \n",
            " [b'00000_of_00004:toydata.tfrecords:079098054'\n",
            " b'00000_of_00005:toydata.tfrecords:24651255884']\n",
            "sequences \n",
            " [[0.39676616 0.8615744  0.8397914 ]\n",
            " [0.22966605 0.5428242  0.48165247]]\n",
            "labels \n",
            " [[0 0 0]\n",
            " [0 0 0]]\n",
            "state_c \n",
            " [[0.96406707 0.63527062 0.02453578 0.55637392 0.61685748 0.62238103\n",
            "  0.01480179 0.77377287]\n",
            " [0.96406707 0.63527062 0.02453578 0.55637392 0.61685748 0.62238103\n",
            "  0.01480179 0.77377287]]\n",
            "state_h \n",
            " [[0.1720632  0.7212568  0.9412366  0.012278   0.63105657 0.17493996\n",
            "  0.12981576 0.53359212]\n",
            " [0.1720632  0.7212568  0.9412366  0.012278   0.63105657 0.17493996\n",
            "  0.12981576 0.53359212]]\n",
            "\n",
            "\n",
            "Iteration 2:\n",
            "keys \n",
            " [b'00001_of_00004:toydata.tfrecords:079098054'\n",
            " b'00001_of_00005:toydata.tfrecords:24651255884']\n",
            "sequences \n",
            " [[0.6774552  0.2049884  0.2182948 ]\n",
            " [0.17545146 0.4974389  0.9696964 ]]\n",
            "labels \n",
            " [[0 0 1]\n",
            " [0 0 0]]\n",
            "state_c \n",
            " [[1.96406707 1.63527062 1.02453578 1.55637392 1.61685748 1.62238103\n",
            "  1.01480179 1.77377287]\n",
            " [1.96406707 1.63527062 1.02453578 1.55637392 1.61685748 1.62238103\n",
            "  1.01480179 1.77377287]]\n",
            "state_h \n",
            " [[1.1720632  1.7212568  1.9412366  1.012278   1.63105657 1.17493996\n",
            "  1.12981576 1.53359212]\n",
            " [1.1720632  1.7212568  1.9412366  1.012278   1.63105657 1.17493996\n",
            "  1.12981576 1.53359212]]\n",
            "\n",
            "\n",
            "Iteration 3:\n",
            "keys \n",
            " [b'00002_of_00004:toydata.tfrecords:079098054'\n",
            " b'00002_of_00005:toydata.tfrecords:24651255884']\n",
            "sequences \n",
            " [[0.23537935 0.12690887 0.14148605]\n",
            " [0.30970794 0.5749386  0.36068693]]\n",
            "labels \n",
            " [[1 1 1]\n",
            " [0 1 1]]\n",
            "state_c \n",
            " [[2.96406707 2.63527062 2.02453578 2.55637392 2.61685748 2.62238103\n",
            "  2.01480179 2.77377287]\n",
            " [2.96406707 2.63527062 2.02453578 2.55637392 2.61685748 2.62238103\n",
            "  2.01480179 2.77377287]]\n",
            "state_h \n",
            " [[2.1720632  2.7212568  2.9412366  2.012278   2.63105657 2.17493996\n",
            "  2.12981576 2.53359212]\n",
            " [2.1720632  2.7212568  2.9412366  2.012278   2.63105657 2.17493996\n",
            "  2.12981576 2.53359212]]\n",
            "\n",
            "\n",
            "Iteration 4:\n",
            "keys \n",
            " [b'00003_of_00004:toydata.tfrecords:079098054'\n",
            " b'00003_of_00005:toydata.tfrecords:24651255884']\n",
            "sequences \n",
            " [[0.9554028  0.5423979  0.39336252]\n",
            " [0.08612079 0.7850748  0.52802724]]\n",
            "labels \n",
            " [[1 1 1]\n",
            " [1 1 1]]\n",
            "state_c \n",
            " [[3.96406707 3.63527062 3.02453578 3.55637392 3.61685748 3.62238103\n",
            "  3.01480179 3.77377287]\n",
            " [3.96406707 3.63527062 3.02453578 3.55637392 3.61685748 3.62238103\n",
            "  3.01480179 3.77377287]]\n",
            "state_h \n",
            " [[3.1720632  3.7212568  3.9412366  3.012278   3.63105657 3.17493996\n",
            "  3.12981576 3.53359212]\n",
            " [3.1720632  3.7212568  3.9412366  3.012278   3.63105657 3.17493996\n",
            "  3.12981576 3.53359212]]\n",
            "\n",
            "\n",
            "Iteration 5:\n",
            "keys \n",
            " [b'00004_of_00005:toydata.tfrecords:24651255884'\n",
            " b'00000_of_00004:toydata.tfrecords:52753498458']\n",
            "sequences \n",
            " [[0.9196907  0.6005745  0.        ]\n",
            " [0.13858429 0.42406717 0.06692021]]\n",
            "labels \n",
            " [[1 1 0]\n",
            " [0 0 0]]\n",
            "state_c \n",
            " [[4.96406707 4.63527062 4.02453578 4.55637392 4.61685748 4.62238103\n",
            "  4.01480179 4.77377287]\n",
            " [0.96406707 0.63527062 0.02453578 0.55637392 0.61685748 0.62238103\n",
            "  0.01480179 0.77377287]]\n",
            "state_h \n",
            " [[4.1720632  4.7212568  4.9412366  4.012278   4.63105657 4.17493996\n",
            "  4.12981576 4.53359212]\n",
            " [0.1720632  0.7212568  0.9412366  0.012278   0.63105657 0.17493996\n",
            "  0.12981576 0.53359212]]\n",
            "\n",
            "\n",
            "Iteration 6:\n",
            "keys \n",
            " [b'00001_of_00004:toydata.tfrecords:52753498458'\n",
            " b'00000_of_00005:toydata.tfrecords:77364300875']\n",
            "sequences \n",
            " [[0.8148959  0.37393337 0.94317967]\n",
            " [0.22425017 0.9721088  0.86698955]]\n",
            "labels \n",
            " [[0 0 0]\n",
            " [0 0 0]]\n",
            "state_c \n",
            " [[1.96406707 1.63527062 1.02453578 1.55637392 1.61685748 1.62238103\n",
            "  1.01480179 1.77377287]\n",
            " [0.96406707 0.63527062 0.02453578 0.55637392 0.61685748 0.62238103\n",
            "  0.01480179 0.77377287]]\n",
            "state_h \n",
            " [[1.1720632  1.7212568  1.9412366  1.012278   1.63105657 1.17493996\n",
            "  1.12981576 1.53359212]\n",
            " [0.1720632  0.7212568  0.9412366  0.012278   0.63105657 0.17493996\n",
            "  0.12981576 0.53359212]]\n",
            "\n",
            "\n",
            "Iteration 7:\n",
            "keys \n",
            " [b'00002_of_00004:toydata.tfrecords:52753498458'\n",
            " b'00001_of_00005:toydata.tfrecords:77364300875']\n",
            "sequences \n",
            " [[0.99856734 0.8969823  0.48522192]\n",
            " [0.2286461  0.12806216 0.24681088]]\n",
            "labels \n",
            " [[1 1 1]\n",
            " [0 0 0]]\n",
            "state_c \n",
            " [[2.96406707 2.63527062 2.02453578 2.55637392 2.61685748 2.62238103\n",
            "  2.01480179 2.77377287]\n",
            " [1.96406707 1.63527062 1.02453578 1.55637392 1.61685748 1.62238103\n",
            "  1.01480179 1.77377287]]\n",
            "state_h \n",
            " [[2.1720632  2.7212568  2.9412366  2.012278   2.63105657 2.17493996\n",
            "  2.12981576 2.53359212]\n",
            " [1.1720632  1.7212568  1.9412366  1.012278   1.63105657 1.17493996\n",
            "  1.12981576 1.53359212]]\n",
            "\n",
            "\n",
            "Iteration 8:\n",
            "keys \n",
            " [b'00003_of_00004:toydata.tfrecords:52753498458'\n",
            " b'00002_of_00005:toydata.tfrecords:77364300875']\n",
            "sequences \n",
            " [[0.2183444  0.48269078 0.4796787 ]\n",
            " [0.607604   0.9674614  0.49992785]]\n",
            "labels \n",
            " [[1 1 1]\n",
            " [1 1 1]]\n",
            "state_c \n",
            " [[3.96406707 3.63527062 3.02453578 3.55637392 3.61685748 3.62238103\n",
            "  3.01480179 3.77377287]\n",
            " [2.96406707 2.63527062 2.02453578 2.55637392 2.61685748 2.62238103\n",
            "  2.01480179 2.77377287]]\n",
            "state_h \n",
            " [[3.1720632  3.7212568  3.9412366  3.012278   3.63105657 3.17493996\n",
            "  3.12981576 3.53359212]\n",
            " [2.1720632  2.7212568  2.9412366  2.012278   2.63105657 2.17493996\n",
            "  2.12981576 2.53359212]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OxU4dx1QGQrX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Application to Model Training\n",
        "\n",
        "In order to illustrate the usage of `batch_sequence_with_states` in the training of a model, a few other under-documented black boxes in TensorFlow are used.\n",
        "Mainly, `batch_sequence_with_states` and `tf.nn.static_state_saving_rnn` make a perfect match.\n",
        "The former not only generates input data for the latter, but also provide spaces tracking the states of the LSTM layer.\n",
        "Formally, the latter takes the former as the value of the argument `state_saver`"
      ]
    },
    {
      "metadata": {
        "id": "X3TappGG5nU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm_size = 64\n",
        "states =  {\n",
        "    'lstm_state_c': np.random.rand(lstm_size),\n",
        "    'lstm_state_h': np.random.rand(lstm_size)\n",
        "}\n",
        "batch = tf.contrib.training.batch_sequences_with_states(\n",
        "    input_key=seq_key,\n",
        "    input_sequences=sequence,\n",
        "    input_context=ctx,\n",
        "    input_length=tf.shape(sequence[F_TOKEN_ID])[0],\n",
        "    initial_states=states,\n",
        "    num_unroll=num_unroll,\n",
        "    batch_size=batch_size,\n",
        "    allow_small_batch=False,\n",
        "    make_keys_unique=True,\n",
        "    make_keys_unique_seed=29392)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgcotWl8iHto",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputs = tf.cast(batch.sequences[F_TOKEN_ID], tf.float64)\n",
        "inputs_by_time = tf.split(value=inputs, num_or_size_splits=num_unroll, axis=1)\n",
        "assert len(inputs_by_time) == num_unroll\n",
        "assert inputs_by_time[0].get_shape() == (batch_size, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGQ_3bhGgX2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM layer\n",
        "cell = tf.nn.rnn_cell.LSTMCell(num_units=lstm_size, reuse=tf.AUTO_REUSE)\n",
        "lstm_output, _ = tf.nn.static_state_saving_rnn(\n",
        "    cell, inputs_by_time, batch, state_name=('lstm_state_c', 'lstm_state_h'))\n",
        "# print(len(lstm_output), lstm_output[0].get_shape())\n",
        "\n",
        "# Adding Dropout\n",
        "lstm_output = tf.nn.dropout(lstm_output, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wfElzdubh45N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logits layer\n",
        "layer_size = 32\n",
        "dense = tf.keras.models.Sequential(name='logits')\n",
        "dense.add(\n",
        "    tf.keras.layers.Dense(layer_size, activation='relu', input_dim=lstm_size))\n",
        "dense.add(\n",
        "    tf.keras.layers.Dense(layer_size, activation='relu'))\n",
        "dense.add(tf.keras.layers.Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yREf1qcje_H1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = tf.transpose(batch.sequences[F_LABEL_ID])\n",
        "logits = dense(tf.cast(lstm_output, tf.float32))\n",
        "logits = tf.squeeze(logits, -1)\n",
        "assert logits.get_shape() == labels.get_shape()\n",
        "\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "    logits=logits, labels=tf.cast(labels, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C40-l0jxp33j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.01)\n",
        "train_op = optimizer.minimize(loss)\n",
        "with tf.control_dependencies([train_op]):\n",
        "    loss_val = tf.reduce_sum(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hXJdElhblbr3",
        "colab_type": "code",
        "outputId": "34165590-345e-4fc5-a130-60a75b80b9ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "len_phase = 1000\n",
        "sum_loss = 0\n",
        "\n",
        "initializer = tf.initializers.global_variables()\n",
        "with tf.Session() as sess:\n",
        "    coord = tf.train.Coordinator()\n",
        "    tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "    sess.run(initializer)\n",
        "    for i in range(1, 30001):\n",
        "        sum_loss += sess.run(loss_val)\n",
        "        if i % len_phase == 0:\n",
        "            print('Mean loss at step {0:5}:  {1}'.format(i, sum_loss / len_phase))\n",
        "            sum_loss = 0\n",
        "    coord.request_stop()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean loss at step  1000:  4.066202268600464\n",
            "Mean loss at step  2000:  4.019917325496674\n",
            "Mean loss at step  3000:  3.9779060764312746\n",
            "Mean loss at step  4000:  3.9280554127693175\n",
            "Mean loss at step  5000:  3.8788565654754636\n",
            "Mean loss at step  6000:  3.8282575843334197\n",
            "Mean loss at step  7000:  3.781137935400009\n",
            "Mean loss at step  8000:  3.714286876440048\n",
            "Mean loss at step  9000:  3.6556279363632203\n",
            "Mean loss at step 10000:  3.595656502485275\n",
            "Mean loss at step 11000:  3.5299532765150072\n",
            "Mean loss at step 12000:  3.4480678269863128\n",
            "Mean loss at step 13000:  3.3744378654956817\n",
            "Mean loss at step 14000:  3.284869670748711\n",
            "Mean loss at step 15000:  3.183376985669136\n",
            "Mean loss at step 16000:  3.1031557419300078\n",
            "Mean loss at step 17000:  2.9778473482728005\n",
            "Mean loss at step 18000:  2.8747157000303267\n",
            "Mean loss at step 19000:  2.7662972851991654\n",
            "Mean loss at step 20000:  2.66409159809351\n",
            "Mean loss at step 21000:  2.573668681561947\n",
            "Mean loss at step 22000:  2.4652993575334547\n",
            "Mean loss at step 23000:  2.403736570686102\n",
            "Mean loss at step 24000:  2.3414405554533007\n",
            "Mean loss at step 25000:  2.271452523678541\n",
            "Mean loss at step 26000:  2.215707157969475\n",
            "Mean loss at step 27000:  2.2136466459929944\n",
            "Mean loss at step 28000:  2.146589555040002\n",
            "Mean loss at step 29000:  2.1627050947844983\n",
            "Mean loss at step 30000:  2.121103879764676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K3JS5tAINxSx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}